{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bec191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks, models, layers, preprocessing as TFprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bbb7ec",
   "metadata": {},
   "source": [
    "Loads the dataset CSV file the specified file path using the pandas library's read_csv() function. The encoding is set to 'latin-1' to ensure that all characters in the CSV file are properly read.\n",
    "\n",
    "Note: This data is preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d01e8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>administration union territory daman diu ha re...</td>\n",
       "      <td>daman diu revoke mandatory rakshabandhan offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>malaika arora slammed instagram user trolled d...</td>\n",
       "      <td>malaika slam user trolled divorcing rich man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indira gandhi institute medical science igims ...</td>\n",
       "      <td>virgin corrected unmarried igims form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lashkaretaibas kashmir commander abu dujana wa...</td>\n",
       "      <td>aaj aapne pakad liya let man dujana killed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotel maharashtra train staff spot sign sex tr...</td>\n",
       "      <td>hotel staff get training spot sign sex traffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32yearold man wednesday wa found hanging insid...</td>\n",
       "      <td>man found dead delhi police station kin allege...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>delhi high court reduced compensation awarded ...</td>\n",
       "      <td>delhi hc reduces aid negligent accident victim 45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60year old dalit woman wa allegedly lynched ag...</td>\n",
       "      <td>60yrold lynched rumour wa cutting people hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inquiry aircraft accident investigation bureau...</td>\n",
       "      <td>chopper flying critically low led 2015 bombay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>congress party ha opened bank called state ban...</td>\n",
       "      <td>congress open state bank tomato lucknow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  administration union territory daman diu ha re...   \n",
       "1  malaika arora slammed instagram user trolled d...   \n",
       "2  indira gandhi institute medical science igims ...   \n",
       "3  lashkaretaibas kashmir commander abu dujana wa...   \n",
       "4  hotel maharashtra train staff spot sign sex tr...   \n",
       "5  32yearold man wednesday wa found hanging insid...   \n",
       "6  delhi high court reduced compensation awarded ...   \n",
       "7  60year old dalit woman wa allegedly lynched ag...   \n",
       "8  inquiry aircraft accident investigation bureau...   \n",
       "9  congress party ha opened bank called state ban...   \n",
       "\n",
       "                                             summary  \n",
       "0  daman diu revoke mandatory rakshabandhan offic...  \n",
       "1       malaika slam user trolled divorcing rich man  \n",
       "2              virgin corrected unmarried igims form  \n",
       "3         aaj aapne pakad liya let man dujana killed  \n",
       "4  hotel staff get training spot sign sex traffic...  \n",
       "5  man found dead delhi police station kin allege...  \n",
       "6  delhi hc reduces aid negligent accident victim 45  \n",
       "7      60yrold lynched rumour wa cutting people hair  \n",
       "8  chopper flying critically low led 2015 bombay ...  \n",
       "9            congress open state bank tomato lucknow  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'C:\\Users\\moeme\\Desktop\\Text Summariztation Dataset\\final_dataset.csv', encoding='latin-1')\n",
    "dataset = dataset.loc[:, ~dataset.columns.str.contains('^Unnamed')]\n",
    "display(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24967221",
   "metadata": {},
   "source": [
    "This block of code performs text preprocessing tasks for a dataset, including tokenization and sequence padding, using TensorFlow libraries.\n",
    "\n",
    "The first three lines of code initialize a tokenizer object, fit it on the text data from the \"text\" column of the dataset, and create a vocabulary dictionary containing the unique words in the text data, with \"<PAD>\" token assigned to 0.\n",
    "\n",
    "The next two lines of code convert the text data into sequences of integers using the texts_to_sequences() method of the tokenizer object and stores them in the variable text2seq.\n",
    "\n",
    "Finally, the last line of code pads the sequences to a fixed length of 1024 and stores them in the text variable. This is achieved using the pad_sequences() function from the sequence module of the TensorFlow package. The resulting sequences can be used for text summarization or other NLP tasks that require fixed-length input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c636177",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = TFprocessing.text.Tokenizer(num_words=10000, lower=False, split=' ', oov_token=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "text_tokenizer.fit_on_texts(dataset[\"text\"])\n",
    "text_dic_vocabulary = {\"<PAD>\":0}\n",
    "text_dic_vocabulary.update(text_tokenizer.word_index)\n",
    "\n",
    "## text to seq\n",
    "text2seq= text_tokenizer.texts_to_sequences(dataset[\"text\"])\n",
    "\n",
    "## padding sequence\n",
    "text = TFprocessing.sequence.pad_sequences(text2seq, maxlen=1024, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2cc85",
   "metadata": {},
   "source": [
    "This block of code adds START and END tokens to the summaries (y) in the dataset. It then applies tokenization and sequence padding on the summary data similar to the previous block of code.\n",
    "\n",
    "The first three lines of the code add \"<START>\" and \"<END>\" tokens to the summaries in the dataset by concatenating them to the beginning and end of each summary using a lambda function applied to the \"summary\" column of the dataset.\n",
    "\n",
    "The next four lines of code are similar to the previous block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7be7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> malaika slam user trolled divorcing rich man <END>\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens to the summaries (y)\n",
    "special_tokens = (\"<START>\", \"<END>\")\n",
    "dataset[\"summary\"] = dataset['summary'].apply(lambda x: special_tokens[0]+' '+x+' '+special_tokens[1])\n",
    "# check example\n",
    "print(dataset[\"summary\"][1])\n",
    "\n",
    "summary_tokenizer = TFprocessing.text.Tokenizer(num_words=10000, lower=False, split=' ', oov_token=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "summary_tokenizer.fit_on_texts(dataset[\"summary\"])\n",
    "summary_dic_vocabulary = {\"<PAD>\":0}\n",
    "summary_dic_vocabulary.update(summary_tokenizer.word_index)\n",
    "\n",
    "## summary to seq\n",
    "summary2seq = summary_tokenizer.texts_to_sequences(dataset[\"summary\"])\n",
    "## padding sequence\n",
    "summary = TFprocessing.sequence.pad_sequences(summary2seq, maxlen=256, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae9a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1720 1167  842 ...    0    0    0]\n",
      "[   1 7620 1428 6458 1051  539 1121 9093  148  338    2    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# check example for the previous two block of code\n",
    "print(text[8])\n",
    "print(summary[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da604a6",
   "metadata": {},
   "source": [
    "This block of code uses the train_test_split() function from scikit-learn library to split the preprocessed text and summary data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c896669",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, summary, test_size=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb851e",
   "metadata": {},
   "source": [
    "This block of code defines a Seq2Seq model using Keras. The model architecture consists of an encoder and a decoder. The encoder consists of an input layer, an embedding layer, and an LSTM layer. The decoder consists of another input layer, an embedding layer, an LSTM layer, and a dense layer. The model takes two inputs: the input sequence (x) and the target sequence (y), and outputs the predicted target sequence (y). The model is compiled with the RMSprop optimizer and sparse categorical cross-entropy loss function, and accuracy is used as a metric. The model summary is displayed at the end.\n",
    "\n",
    "As you see in model summary there is 396,732,106 trainable parameters in on the model. \n",
    "\n",
    "Understand RNN and LSTM:\n",
    "\n",
    "[LSTM](https://drive.google.com/file/d/1gSBaOkAWy9JGfp6IgZahw5_1hbaJV-b6/view?usp=sharing)\n",
    "\n",
    "[LSTM Lecture](https://drive.google.com/file/d/1aC1NR3aw3AzrOKkbORfr0HDiy0GcV4JA/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac84544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Seq2Seq\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x_input_layer (InputLayer)     [(None, 1024)]       0           []                               \n",
      "                                                                                                  \n",
      " y_input_layer (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " x_embedding_layer (Embedding)  (None, 1024, 300)    268015200   ['x_input_layer[0][0]']          \n",
      "                                                                                                  \n",
      " y_embedding_layer (Embedding)  (None, None, 300)    69481800    ['y_input_layer[0][0]']          \n",
      "                                                                                                  \n",
      " x_lstm_layer (LSTM)            [(None, 1024, 250),  551000      ['x_embedding_layer[0][0]']      \n",
      "                                 (None, 250),                                                     \n",
      "                                 (None, 250)]                                                     \n",
      "                                                                                                  \n",
      " y_lstm_layer (LSTM)            [(None, None, 250),  551000      ['y_embedding_layer[0][0]',      \n",
      "                                 (None, 250),                     'x_lstm_layer[0][1]',           \n",
      "                                 (None, 250)]                     'x_lstm_layer[0][2]']           \n",
      "                                                                                                  \n",
      " dense (TimeDistributed)        (None, None, 231606  58133106    ['y_lstm_layer[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 396,732,106\n",
      "Trainable params: 396,732,106\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_units = 250\n",
    "embeddings_size = 300\n",
    "##------------ ENCODER (embedding + lstm) ------------------------##\n",
    "x_input_layer = layers.Input(name=\"x_input_layer\", shape=(X_train.shape[1],))\n",
    "### embedding\n",
    "x_embedding = layers.Embedding(name=\"x_embedding_layer\", input_dim=len(text_dic_vocabulary),output_dim=embeddings_size, trainable=True)\n",
    "embedding_layer = x_embedding(x_input_layer)\n",
    "\n",
    "### lstm \n",
    "x_lstm_layer = layers.LSTM(name=\"x_lstm_layer\", units=lstm_units, dropout=0.4, return_sequences=True, return_state=True)\n",
    "x_out, state_h, state_c = x_lstm_layer(embedding_layer)\n",
    "\n",
    "##------------ DECODER (embedding + lstm + dense) ----------------##\n",
    "y_input_layer = layers.Input(name=\"y_input_layer\", shape=(None,))\n",
    "\n",
    "### embedding\n",
    "y_embedding = layers.Embedding(name=\"y_embedding_layer\", input_dim=len(summary_dic_vocabulary), output_dim=embeddings_size, trainable=True)\n",
    "y_embedding_layer = y_embedding(y_input_layer)\n",
    "\n",
    "### lstm \n",
    "y_lstm_layer = layers.LSTM(name=\"y_lstm_layer\", units=lstm_units, dropout=0.4, return_sequences=True, return_state=True)\n",
    "y_out, _, _ = y_lstm_layer(y_embedding_layer, initial_state=[state_h, state_c])\n",
    "\n",
    "### final dense layers\n",
    "dense_layer = layers.TimeDistributed(name=\"dense\", layer=layers.Dense(units=len(summary_dic_vocabulary), activation='softmax'))\n",
    "y_out = dense_layer(y_out)\n",
    "\n",
    "##---------------------------- COMPILE ---------------------------##\n",
    "model = models.Model(inputs=[x_input_layer, y_input_layer], outputs=y_out, name=\"Seq2Seq\")\n",
    "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547399f",
   "metadata": {},
   "source": [
    "This block of code trains the Seq2Seq model using the fit() method of the model object. The training is performed using the input data X_train and target data y_train. The target data is first reshaped to remove the first column, which is the start token. The training is done for 100 epochs, with a batch size of 16. The validation split is set to 0.3, which means 30% of the training data is used for validation. The training progress is displayed using the verbose parameter. The EarlyStopping callback is also used to stop training if the validation loss does not improve for 2 consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919af0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    6/14521 [..............................] - ETA: 242:21:01 - loss: 10.9173 - accuracy: 0.7563"
     ]
    }
   ],
   "source": [
    "## train\n",
    "training = model.fit(x=[X_train, y_train[:,:-1]], \n",
    "                     y=y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:],\n",
    "                     batch_size=16, \n",
    "                     epochs=100,  \n",
    "                     verbose=1, \n",
    "                     validation_split=0.3,\n",
    "                     callbacks=[callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot loss and accuracy\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
